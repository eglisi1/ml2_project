{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"config/config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "filepath = config[\"kaggle\"][\"api_filepath\"]\n",
    "\n",
    "# Load Kaggle API token\n",
    "try:\n",
    "    with open(file=filepath) as f:\n",
    "        api_token = json.load(fp=f)\n",
    "except FileNotFoundError:\n",
    "    print(f\"File '{filepath}' not found.\")\n",
    "    print(\n",
    "        f\"Download the API key from Kaggle and save to {filepath} or adjust the config.yaml as necessary.\"\n",
    "    )\n",
    "    print(f\"See https://www.kaggle.com/docs/api for more information.\")\n",
    "    raise FileNotFoundError  # Stop execution of the script in order to signal the user to fix the issue.\n",
    "\n",
    "# Define environment variables (Kaggle API client expects these)\n",
    "os.environ[\"KAGGLE_USERNAME\"] = api_token[\"username\"]\n",
    "os.environ[\"KAGGLE_KEY\"] = api_token[\"key\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download and unzip the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import time\n",
    "\n",
    "# Initialize the API\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = \"jiashenliu/515k-hotel-reviews-data-in-europe\"\n",
    "download_path = config[\"kaggle\"][\"download_path\"]\n",
    "\n",
    "\n",
    "# Download the dataset\n",
    "start = time.time()\n",
    "api.dataset_download_files(dataset=dataset_path, path=download_path, unzip=True)\n",
    "end = time.time()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filename = download_path + \"/Hotel_Reviews.csv\"\n",
    "\n",
    "start = time.time()\n",
    "df = pd.read_csv(filepath_or_buffer=filename)\n",
    "end = time.time()\n",
    "print(f\"Loading took {round(end - start, 2)} seconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will drop the null rows because we can't replace the lat or lng with mean or median value, that will change the right information hotel adderss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True,axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check for duplicates and drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Duplicated rows before: \", df.duplicated().sum())\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(\"Duplicated rows after: \", df.duplicated().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace all the addresses with a shortened, more useful form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_address(row):\n",
    "    if \"Netherlands\" in row[\"Hotel_Address\"]:\n",
    "        return \"Amsterdam, Netherlands\"\n",
    "    elif \"Barcelona\" in row[\"Hotel_Address\"]:\n",
    "        return \"Barcelona, Spain\"\n",
    "    elif \"United Kingdom\" in row[\"Hotel_Address\"]:\n",
    "        return \"London, United Kingdom\"\n",
    "    elif \"Milan\" in row[\"Hotel_Address\"]:\n",
    "        return \"Milan, Italy\"\n",
    "    elif \"France\" in row[\"Hotel_Address\"]:\n",
    "        return \"Paris, France\"\n",
    "    elif \"Vienna\" in row[\"Hotel_Address\"]:\n",
    "        return \"Vienna, Austria\"\n",
    "    else:\n",
    "        return row.Hotel_Address\n",
    "\n",
    "\n",
    "df[\"Hotel_Address\"] = df.apply(func=replace_address, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace `Total_Number_of_Reviews` and `Average_Score` with own calculated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"Additional_Number_of_Scoring\"], axis=1, inplace=True)\n",
    "df[\"Total_Number_of_Reviews\"] = df.groupby(\"Hotel_Name\")[\"Hotel_Name\"].transform(\n",
    "    \"count\"\n",
    ")\n",
    "df[\"Average_Score\"] = (\n",
    "    df.groupby(\"Hotel_Name\")[\"Reviewer_Score\"].transform(\"mean\").round(1)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the most useful tags from the `Tags` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove opening and closing brackets and all quotes too, split the strings into a list\n",
    "tag_list_df = (\n",
    "    df.Tags.str.strip(\"[']\")\n",
    "    .str.replace(\" ', '\", \",\", regex=False)\n",
    "    .str.split(\",\", expand=True)\n",
    ")\n",
    "\n",
    "# Remove leading and trailing spaces from each column and assign them back to the dataframe\n",
    "for i in range(6):\n",
    "    df[f\"Tag_{i+1}\"] = tag_list_df[i].str.strip()\n",
    "\n",
    "# Merge the 6 columns into one with melt\n",
    "df_tags = df.melt(value_vars=[f\"Tag_{i+1}\" for i in range(6)])\n",
    "\n",
    "# Print the shape of the tags with no filtering\n",
    "print(\"The shape of the tags with no filtering:\", df_tags.shape)\n",
    "\n",
    "# Filter the tags, get the value counts\n",
    "df_tags = df_tags[\n",
    "    ~df_tags.value.str.contains(\n",
    "        \"Standard|room|Stayed|device|Beds|Suite|Studio|King|Superior|Double\",\n",
    "        na=False,\n",
    "        case=False,\n",
    "    )\n",
    "]\n",
    "tag_vc = df_tags.value.value_counts().reset_index(name=\"count\").query(\"count > 1000\")\n",
    "\n",
    "# Print the top 10 (there should only be 9 and we'll use these in the filtering section)\n",
    "tag_vc[\"value\"] = tag_vc[\"value\"].apply(\n",
    "    lambda x: x.strip().replace(\" \", \"_\") if isinstance(x, str) else x\n",
    ")\n",
    "print(tag_vc[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process tags into new columns with one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the Tags into new columns\n",
    "for tag in tag_vc[\"value\"]:\n",
    "    df[tag] = df.Tags.apply(lambda x: 1 if tag in x else 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\n",
    "    [\n",
    "        \"Review_Date\",\n",
    "        \"Review_Total_Negative_Word_Counts\",\n",
    "        \"Review_Total_Positive_Word_Counts\",\n",
    "        \"days_since_review\",\n",
    "        \"Total_Number_of_Reviews_Reviewer_Has_Given\",\n",
    "        \"Tags\",\n",
    "        \"Tag_1\",\n",
    "        \"Tag_2\",\n",
    "        \"Tag_3\",\n",
    "        \"Tag_4\",\n",
    "        \"Tag_5\",\n",
    "        \"Tag_6\",\n",
    "    ],\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA (Exploratory Data Analysis) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since Lea and I also can't decide which city to choose the capital of the language of love is a good start for some romantic activities but also fot the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paris_df = df[df[\"Hotel_Address\"] == \"Paris, France\"]\n",
    "paris_df = paris_df.drop_duplicates(subset=\"Hotel_Name\")\n",
    "paris_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a map of the hotels in Paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def assign_color(score: float) -> str:\n",
    "    if score >= 9.3:\n",
    "        return \"green\"\n",
    "    elif score >= 8.3:\n",
    "        return \"lightgreen\"\n",
    "    elif score >= 7.3:\n",
    "        return \"orange\"\n",
    "    else:\n",
    "        return \"red\"\n",
    "\n",
    "\n",
    "# Initialize map with starting location\n",
    "paris_map = folium.Map(\n",
    "    location=[paris_df[\"lat\"].mean(), paris_df[\"lng\"].mean()], zoom_start=12\n",
    ")\n",
    "\n",
    "for idx, row in tqdm(iterable=paris_df.iterrows(), total=paris_df.shape[0]):\n",
    "    folium.Marker(\n",
    "        location=[row[\"lat\"], row[\"lng\"]],\n",
    "        popup=f'{row[\"Hotel_Name\"]}: {row[\"Average_Score\"]}',\n",
    "        icon=folium.Icon(color=assign_color(row[\"Average_Score\"]), icon=\"info-sign\"),\n",
    "    ).add_to(paris_map)\n",
    "\n",
    "# Show the map\n",
    "paris_map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to calculate sentiment and to remove the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk as nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from keras import models\n",
    "import numpy as np\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk import download\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_review_length = config[\"num_words\"]\n",
    "word2index = imdb.get_word_index()\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"stopwords\")\n",
    "download(\"punkt\")\n",
    "\n",
    "vader_sentiment = SentimentIntensityAnalyzer()\n",
    "cache = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def calc_sentiment_vader(review: str) -> float:\n",
    "    \"\"\"\n",
    "    The `calc_sentiment` function takes a string as input and returns a float representing the sentiment of the input text.\n",
    "    \"\"\"\n",
    "    if review == \"No Negative\" or review == \"No Positive\":\n",
    "        return 0\n",
    "    return vader_sentiment.polarity_scores(text=review)[\"compound\"]\n",
    "\n",
    "\n",
    "def remove_stopwords(review: str) -> str:\n",
    "    \"\"\"\n",
    "    The `remove_stopwords` function takes a string as input and returns a string where all English stopwords have been removed.\n",
    "    It does this by splitting the input text into individual words, filtering out the stopwords,\n",
    "    and then joining the remaining words back together into a single string.\n",
    "    \"\"\"\n",
    "    text = \" \".join([word for word in review.split() if word not in cache])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, name: str, keras_model: models.Sequential):\n",
    "        self.name = name\n",
    "        self.keras_model = keras_model\n",
    "    \n",
    "    def tokenizee(self, review: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        The `tokenizee` function takes a string as input and returns a numpy array of integers.\n",
    "        It's called this way to avoid a name conflict and confusion with the `tokenize` function from the `nltk` package.\n",
    "        \"\"\"\n",
    "        word2index_list = []\n",
    "        for word in word_tokenize(review):\n",
    "            if word.isalpha():  # Only process if word is alphabetical\n",
    "                word = word.lower()  # convert to lower case\n",
    "                if word in word2index:  # only add if word exists in word2index\n",
    "                    word2index_list.append(word2index[word])\n",
    "                else:\n",
    "                    word2index_list.append(0)  # or some other value to denote unknown words\n",
    "        return sequence.pad_sequences([word2index_list], maxlen=max_review_length)\n",
    "\n",
    "    def calc_sentiment(self, review: str) -> float:\n",
    "        \"\"\"\n",
    "        The `calc_sentiment_model` function takes a string as input and returns a float representing the sentiment of the input text.\n",
    "        \"\"\"\n",
    "        if review == \"No Negative\" or review == \"No Positive\":\n",
    "            return 0\n",
    "        review = remove_stopwords(review)\n",
    "        tokenized_review = self.tokenizee(review)\n",
    "        sentiment=self.keras_model.predict([tokenized_review], verbose=0)[0][0]\n",
    "        if sentiment == 0:\n",
    "            sentiment = -1.0\n",
    "        return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "loaded_models: List[Model] = []\n",
    "\n",
    "# Load the models\n",
    "model_config_dir = os.path.join(os.getcwd(), \"config\", \"model\")\n",
    "model_dir = os.path.join(os.getcwd(), \"model\")\n",
    "for model_config in os.listdir(model_config_dir):\n",
    "    model_config_path = os.path.join(model_config_dir, model_config)\n",
    "    with open(model_config_path, \"r\") as f:\n",
    "        model_config = yaml.safe_load(f)\n",
    "        model_name = model_config[\"model\"][\"name\"]\n",
    "        if \"zzz\" in model_name.lower():\n",
    "            print(f\"Skipping model '{model_name}' in config : {model_config_path}\")\n",
    "            continue\n",
    "        model_path = os.path.join(model_dir, model_name)\n",
    "        keras_model = models.load_model(model_path)\n",
    "        model = Model(model_name, keras_model)\n",
    "        loaded_models.append(model)\n",
    "\n",
    "print(\"all loded models:\")\n",
    "for model in loaded_models:\n",
    "    print(f\"\\tloaded model: {model.name}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Remove the stop words from both columns\n",
    "df.Negative_Review = df.Negative_Review.apply(remove_stopwords)   \n",
    "df.Positive_Review = df.Positive_Review.apply(remove_stopwords)\n",
    "end = time.time()\n",
    "print(\"Removing stop words took \" + str(round(end - start, 2)) + \" seconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a negative sentiment and positive sentiment column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating sentiment columns for both positive and negative reviews\")\n",
    "start = time.time()\n",
    "df[\"Negative_Sentiment_Vader\"] = df.Negative_Review.apply(calc_sentiment_vader)\n",
    "df[\"Positive_Sentiment_Vader\"] = df.Positive_Review.apply(calc_sentiment_vader)\n",
    "end = time.time()\n",
    "print(f\"Calculating sentiment took {(round(end - start, 2))} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).head(100)\n",
    "\n",
    "for model in loaded_models:\n",
    "    print(f\"Calculating sentiment columns for both positive and negative reviews using model '{model.name}'\")\n",
    "    start = time.time()\n",
    "    df[f\"Negative_Sentiment_{model.name}\"] = df.Negative_Review.apply(model.calc_sentiment)\n",
    "    df[f\"Positive_Sentiment_{model.name}\"] = df.Positive_Review.apply(model.calc_sentiment)\n",
    "    end = time.time()\n",
    "    print(f\"Calculating sentiment took {(round(end - start, 2))} seconds for model '{model.name}'\")\n",
    "    print(\"....................................................\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
