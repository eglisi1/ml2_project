{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "filepath = config[\"kaggle\"][\"api_filepath\"]\n",
    "\n",
    "# Load Kaggle API token\n",
    "try:\n",
    "    with open(file=filepath) as f:\n",
    "        api_token = json.load(fp=f)\n",
    "except FileNotFoundError:\n",
    "    print(f\"File '{filepath}' not found.\")\n",
    "    print(\n",
    "        f\"Download the API key from Kaggle and save to {filepath} or adjust the config.yaml as necessary.\"\n",
    "    )\n",
    "    print(f\"See https://www.kaggle.com/docs/api for more information.\")\n",
    "    raise FileNotFoundError  # Stop execution of the script in order to signal the user to fix the issue.\n",
    "\n",
    "# Define environment variables (Kaggle API client expects these)\n",
    "os.environ[\"KAGGLE_USERNAME\"] = api_token[\"username\"]\n",
    "os.environ[\"KAGGLE_KEY\"] = api_token[\"key\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download and unzip the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import time\n",
    "\n",
    "# Initialize the API\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = \"jiashenliu/515k-hotel-reviews-data-in-europe\"\n",
    "download_path = config[\"kaggle\"][\"download_path\"]\n",
    "\n",
    "\n",
    "# Download the dataset\n",
    "start = time.time()\n",
    "api.dataset_download_files(dataset=dataset_path, path=download_path, unzip=True)\n",
    "end = time.time()\n",
    "print(f\"Time to download and extract: {round(end - start, 2)} seconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filename = download_path + \"/Hotel_Reviews.csv\"\n",
    "\n",
    "start = time.time()\n",
    "df = pd.read_csv(filepath_or_buffer=filename)\n",
    "end = time.time()\n",
    "print(f\"Loading took {round(end - start, 2)} seconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will drop the null rows because we can't replace the lat or lng with mean or median value, that will change the right information hotel adderss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True,axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check for duplicates and drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Duplicated rows before: \", df.duplicated().sum())\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(\"Duplicated rows after: \", df.duplicated().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace all the addresses with a shortened, more useful form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_address(row):\n",
    "    if \"Netherlands\" in row[\"Hotel_Address\"]:\n",
    "        return \"Amsterdam, Netherlands\"\n",
    "    elif \"Barcelona\" in row[\"Hotel_Address\"]:\n",
    "        return \"Barcelona, Spain\"\n",
    "    elif \"United Kingdom\" in row[\"Hotel_Address\"]:\n",
    "        return \"London, United Kingdom\"\n",
    "    elif \"Milan\" in row[\"Hotel_Address\"]:\n",
    "        return \"Milan, Italy\"\n",
    "    elif \"France\" in row[\"Hotel_Address\"]:\n",
    "        return \"Paris, France\"\n",
    "    elif \"Vienna\" in row[\"Hotel_Address\"]:\n",
    "        return \"Vienna, Austria\"\n",
    "    else:\n",
    "        return row.Hotel_Address\n",
    "\n",
    "\n",
    "df[\"Hotel_Address\"] = df.apply(func=replace_address, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace `Total_Number_of_Reviews` and `Average_Score` with own calculated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"Additional_Number_of_Scoring\"], axis=1, inplace=True)\n",
    "df[\"Total_Number_of_Reviews\"] = df.groupby(\"Hotel_Name\")[\"Hotel_Name\"].transform(\n",
    "    \"count\"\n",
    ")\n",
    "df[\"Average_Score\"] = (\n",
    "    df.groupby(\"Hotel_Name\")[\"Reviewer_Score\"].transform(\"mean\").round(1)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the most useful tags from the `Tags` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove opening and closing brackets and all quotes too, split the strings into a list\n",
    "tag_list_df = (\n",
    "    df.Tags.str.strip(\"[']\")\n",
    "    .str.replace(\" ', '\", \",\", regex=False)\n",
    "    .str.split(\",\", expand=True)\n",
    ")\n",
    "\n",
    "# Remove leading and trailing spaces from each column and assign them back to the dataframe\n",
    "for i in range(6):\n",
    "    df[f\"Tag_{i+1}\"] = tag_list_df[i].str.strip()\n",
    "\n",
    "# Merge the 6 columns into one with melt\n",
    "df_tags = df.melt(value_vars=[f\"Tag_{i+1}\" for i in range(6)])\n",
    "\n",
    "# Print the shape of the tags with no filtering\n",
    "print(\"The shape of the tags with no filtering:\", df_tags.shape)\n",
    "\n",
    "# Filter the tags, get the value counts\n",
    "df_tags = df_tags[\n",
    "    ~df_tags.value.str.contains(\n",
    "        \"Standard|room|Stayed|device|Beds|Suite|Studio|King|Superior|Double\",\n",
    "        na=False,\n",
    "        case=False,\n",
    "    )\n",
    "]\n",
    "tag_vc = df_tags.value.value_counts().reset_index(name=\"count\").query(\"count > 1000\")\n",
    "\n",
    "# Print the top 10 (there should only be 9 and we'll use these in the filtering section)\n",
    "tag_vc[\"value\"] = tag_vc[\"value\"].apply(\n",
    "    lambda x: x.strip().replace(\" \", \"_\") if isinstance(x, str) else x\n",
    ")\n",
    "print(tag_vc[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process tags into new columns with one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the Tags into new columns\n",
    "for tag in tag_vc[\"value\"]:\n",
    "    df[tag] = df.Tags.apply(lambda x: 1 if tag in x else 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\n",
    "    [\n",
    "        \"Review_Date\",\n",
    "        \"Review_Total_Negative_Word_Counts\",\n",
    "        \"Review_Total_Positive_Word_Counts\",\n",
    "        \"days_since_review\",\n",
    "        \"Total_Number_of_Reviews_Reviewer_Has_Given\",\n",
    "        \"Tags\",\n",
    "        \"Tag_1\",\n",
    "        \"Tag_2\",\n",
    "        \"Tag_3\",\n",
    "        \"Tag_4\",\n",
    "        \"Tag_5\",\n",
    "        \"Tag_6\",\n",
    "    ],\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA (Exploratory Data Analysis) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since Lea and I also can't decide which city to choose the capital of the language of love is a good start for some romantic activities but also fot the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paris_df = df[df[\"Hotel_Address\"] == \"Paris, France\"]\n",
    "paris_df = paris_df.drop_duplicates(subset=\"Hotel_Name\")\n",
    "paris_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a map of the hotels in Paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def assign_color(score: float) -> str:\n",
    "    if score >= 9.3:\n",
    "        return \"green\"\n",
    "    elif score >= 8.3:\n",
    "        return \"lightgreen\"\n",
    "    elif score >= 7.3:\n",
    "        return \"orange\"\n",
    "    else:\n",
    "        return \"red\"\n",
    "\n",
    "\n",
    "# Initialize map with starting location\n",
    "paris_map = folium.Map(\n",
    "    location=[paris_df[\"lat\"].mean(), paris_df[\"lng\"].mean()], zoom_start=12\n",
    ")\n",
    "\n",
    "for idx, row in tqdm(iterable=paris_df.iterrows(), total=paris_df.shape[0]):\n",
    "    folium.Marker(\n",
    "        location=[row[\"lat\"], row[\"lng\"]],\n",
    "        popup=f'{row[\"Hotel_Name\"]}: {row[\"Average_Score\"]}',\n",
    "        icon=folium.Icon(color=assign_color(row[\"Average_Score\"]), icon=\"info-sign\"),\n",
    "    ).add_to(paris_map)\n",
    "\n",
    "# Show the map\n",
    "paris_map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk as nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from keras.models import load_model\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"stopwords\")\n",
    "model_path = config[\"model\"][\"model_path\"]\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model(model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to calculate sentiment and to remove the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_sentiment = SentimentIntensityAnalyzer()\n",
    "cache = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def calc_sentiment(review: str) -> float:\n",
    "    \"\"\"\n",
    "    The `calc_sentiment` function takes a string as input and returns a float representing the sentiment of the input text.\n",
    "    \"\"\"\n",
    "    if review == \"No Negative\" or review == \"No Positive\":\n",
    "        return 0\n",
    "    return vader_sentiment.polarity_scores(text=review)[\"compound\"]\n",
    "\n",
    "\n",
    "def remove_stopwords(review: str) -> str:\n",
    "    \"\"\"\n",
    "    The `remove_stopwords` function takes a string as input and returns a string where all English stopwords have been removed.\n",
    "    It does this by splitting the input text into individual words, filtering out the stopwords,\n",
    "    and then joining the remaining words back together into a single string.\n",
    "    \"\"\"\n",
    "    text = \" \".join([word for word in review.split() if word not in cache])\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Remove the stop words from both columns\n",
    "df.Negative_Review = df.Negative_Review.apply(remove_stopwords)   \n",
    "df.Positive_Review = df.Positive_Review.apply(remove_stopwords)\n",
    "end = time.time()\n",
    "print(\"Removing stop words took \" + str(round(end - start, 2)) + \" seconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a negative sentiment and positive sentiment column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating sentiment columns for both positive and negative reviews\")\n",
    "start = time.time()\n",
    "df[\"Negative_Sentiment\"] = df.Negative_Review.apply(calc_sentiment)\n",
    "df[\"Positive_Sentiment\"] = df.Positive_Review.apply(calc_sentiment)\n",
    "end = time.time()\n",
    "print(f\"Calculating sentiment took {(round(end - start, 2))} seconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the columns ascending by negative sentiment and positive sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=[\"Negative_Sentiment\"], ascending=True)\n",
    "print(df[[\"Negative_Review\", \"Negative_Sentiment\"]])\n",
    "df = df.sort_values(by=[\"Positive_Sentiment\"], ascending=True)\n",
    "print(df[[\"Positive_Review\", \"Positive_Sentiment\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder the columns (This is cosmetic, but to make it easier to explore the data later)\n",
    "df = df.reindex(\n",
    "    [\n",
    "        \"Hotel_Name\",\n",
    "        \"Hotel_Address\",\n",
    "        \"Total_Number_of_Reviews\",\n",
    "        \"Average_Score\",\n",
    "        \"Reviewer_Score\",\n",
    "        \"Negative_Sentiment\",\n",
    "        \"Positive_Sentiment\",\n",
    "        \"Reviewer_Nationality\",\n",
    "        \"Leisure_trip\",\n",
    "        \"Couple\",\n",
    "        \"Solo_traveler\",\n",
    "        \"Business_trip\",\n",
    "        \"Group\",\n",
    "        \"Family_with_young_children\",\n",
    "        \"Family_with_older_children\",\n",
    "        \"With_a_pet\",\n",
    "        \"Negative_Review\",\n",
    "        \"Positive_Review\",\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
